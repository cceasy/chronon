package ai.chronon.integrations.cloud_gcp

import ai.chronon.spark.submission.{JobSubmitter, JobType}
import com.google.cloud.dataproc.v1._
import ai.chronon.spark.submission.JobSubmitterConstants._
import com.google.api.gax.rpc.ApiException

import scala.jdk.CollectionConverters._

class DataprocServerlessSubmitter(batchControllerClient: BatchControllerClient,
                                  val region: String,
                                  val projectId: String)
    extends JobSubmitter {

  override def submit(jobType: JobType,
                      submissionProperties: Map[String, String],
                      jobProperties: Map[String, String],
                      files: List[String],
                      labels: Map[String, String],
                      args: String*): String = {
    val mainClass = submissionProperties.getOrElse(MainClass, throw new RuntimeException("Main class not found"))
    val jarUri = submissionProperties.getOrElse(JarURI, throw new RuntimeException("Jar URI not found"))
    val jobId = submissionProperties.getOrElse(JobId, throw new RuntimeException("No generated job id found"))

    // Format labels following Dataproc standards using shared utility
    val formattedDataprocLabels = DataprocUtils.createFormattedDataprocLabels(
      jobType = jobType,
      submissionProperties = submissionProperties,
      additionalLabels = labels
    )

    val batch = buildBatch(mainClass, jarUri, files, jobProperties, formattedDataprocLabels, args: _*)
    val locationName = LocationName.of(projectId, region)

    logger.info(s"Creating batch with ID: $jobId in location: $locationName")
    logger.info(s"Batch details: ${batch.toString}")

    try {
      val batchF = batchControllerClient.createBatchAsync(locationName, batch, jobId)
      val result = batchF.get
      logger.info(s"Batch created successfully: ${result.getName}")
      logger.info(s"Batch state: ${result.getState}")
      result.getUuid
    } catch {
      case ex: java.util.concurrent.ExecutionException =>
        logger.error(s"ExecutionException during batch creation", ex)
        throw new RuntimeException(s"Failed to create batch: ${ex.getMessage}", ex)
      case ex: ApiException =>
        logger.error(s"ApiException during batch creation", ex)
        throw new RuntimeException(s"Failed to create batch: ${ex.getMessage}", ex)
    }
  }

  override def status(jobId: String): String = {
    try {
      val batchName = s"projects/$projectId/locations/$region/batches/$jobId"
      val batch = batchControllerClient.getBatch(batchName)
      batch.getState.toString
    } catch {
      case e: ApiException =>
        logger.error(s"Error getting batch status: ${e.getMessage}")
        "UNKNOWN"
    }
  }

  override def kill(jobId: String): Unit = {
    try {
      val batchName = s"projects/$projectId/locations/$region/batches/$jobId"
      batchControllerClient.deleteBatch(batchName)
      logger.info(s"Batch $jobId deletion requested")
    } catch {
      case e: ApiException =>
        logger.error(s"Error deleting batch: ${e.getMessage}", e)
        throw new RuntimeException(s"Failed to delete batch: ${e.getMessage}", e)
    }
  }

  private def buildBatch(mainClass: String,
                         jarUri: String,
                         files: List[String],
                         jobProperties: Map[String, String],
                         labels: Map[String, String],
                         args: String*): Batch = {
    val sparkJob = SparkBatch
      .newBuilder()
      .setMainClass(mainClass)
      .addJarFileUris(jarUri)
      .addAllFileUris(files.asJava)
      .addAllArgs(args.toIterable.asJava)
      .build()

    val batchBuilder = Batch
      .newBuilder()
      .setSparkBatch(sparkJob)

    // Add RuntimeConfig only if we have job properties
    // Minimal Dataproc Serverless batches don't require RuntimeConfig
    val runtimeConfBuilder = RuntimeConfig
      .newBuilder()
      .setVersion("1.2")
    if (jobProperties.nonEmpty) {
      runtimeConfBuilder
        .putAllProperties(jobProperties.asJava)
    }
    batchBuilder.setRuntimeConfig(runtimeConfBuilder.build())

    // Add EnvironmentConfig with ExecutionConfig to specify service account
    val executionConfig = ExecutionConfig
      .newBuilder()
      .setServiceAccount(s"dataproc@${projectId}.iam.gserviceaccount.com")
      .build()

    val environmentConfig = EnvironmentConfig
      .newBuilder()
      .setExecutionConfig(executionConfig)
      .build()

    batchBuilder
      .setEnvironmentConfig(environmentConfig)
      .putAllLabels(labels.asJava)
      .build()
  }
}

object DataprocServerlessSubmitter {
  def apply(batchControllerClient: BatchControllerClient,
            region: String,
            projectId: String): DataprocServerlessSubmitter = {
    new DataprocServerlessSubmitter(batchControllerClient, region, projectId)
  }
}
